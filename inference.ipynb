{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8edc4785-c216-4867-97bb-2e11d91dcc46",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#File descirption: Performs inference and visualizes predictions\n",
        "import torch\n",
        "from torch import nn\n",
        "from step1 import test_dataloader\n",
        "from step2 import ObjectDetectionNetwork  # Ensure correct import\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from torchvision.transforms import ToPILImage\n",
        "import torchvision.ops as ops  # For Non-Maximum Suppression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e64f1088-acb0-41bb-bdfb-b6bbc86db032",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6b808fb3-cc06-4093-9e70-88a1e9840b63",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Label mapping (matches Step3 training)\n",
        "category_map = {\n",
        "    'person': 0, 'rider': 1, 'car': 2, 'truck': 3, 'bus': 4,\n",
        "    'train': 5, 'motorcycle': 6, 'bicycle': 7, 'traffic light': 8, 'traffic sign': 9\n",
        "}\n",
        "inverse_category_map = {v: k for k, v in category_map.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "08dbfd0a-6b42-4d53-918c-cfd8f6c35100",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Model setup\n",
        "model = ObjectDetectionNetwork(num_classes=10).to(device)\n",
        "checkpoint_path = \"model_epoch_10.pth\"  # Adjust to desired epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bdb1a485-b43a-460c-98d0-e51efc0210b9",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "Checkpoint model_epoch_10.pth not found!",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded checkpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheckpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m model.eval()\n",
            "\u001b[31mFileNotFoundError\u001b[39m: Checkpoint model_epoch_10.pth not found!"
          ]
        }
      ],
      "source": [
        "# Load checkpoint\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint)\n",
        "    print(f\"Loaded checkpoint: {checkpoint_path}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Checkpoint {checkpoint_path} not found!\")\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e72d1e7-7e5e-4a37-bcc6-1d0f122f0975",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Inference function with visualization\n",
        "def perform_inference_and_visualize(model, dataloader, confidence_threshold=0.5, num_images_to_show=3):\n",
        "    predictions = []\n",
        "    to_pil = ToPILImage()  # Convert tensor to PIL for plotting\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in dataloader:\n",
        "            try:\n",
        "                images = torch.stack(images).to(device)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            # Model prediction\n",
        "            class_logits, box_logits = model(images)  # [batch, 10, 32, 32], [batch, 4, 32, 32]\n",
        "            batch_size = images.size(0)\n",
        "\n",
        "            # Apply softmax to class logits for probabilities\n",
        "            class_probs = torch.softmax(class_logits, dim=1)  # [batch, 10, 32, 32]\n",
        "            max_probs, pred_classes = class_probs.max(dim=1)  # [batch, 32, 32]\n",
        "\n",
        "            # Process and visualize each image in the batch\n",
        "            for b in range(min(batch_size, num_images_to_show)):\n",
        "                img_preds = []\n",
        "                boxes = []\n",
        "                scores = []\n",
        "                labels = []\n",
        "                \n",
        "                # Collect all predictions for this image\n",
        "                for i in range(32):\n",
        "                    for j in range(32):\n",
        "                        prob = max_probs[b, i, j].item()\n",
        "                        class_id = pred_classes[b, i, j].item()\n",
        "                        \n",
        "                        if prob < confidence_threshold or class_id == 9:\n",
        "                            continue  # Skip low confidence and background\n",
        "                        \n",
        "                        # Get box coordinates (absolute in 256x256 space)\n",
        "                        cx = box_logits[b, 0, i, j].item()\n",
        "                        cy = box_logits[b, 1, i, j].item()\n",
        "                        w = box_logits[b, 2, i, j].item()\n",
        "                        h = box_logits[b, 3, i, j].item()\n",
        "                        \n",
        "                        # Convert to x1y1x2y2 format\n",
        "                        x1 = cx - w/2\n",
        "                        y1 = cy - h/2\n",
        "                        x2 = cx + w/2\n",
        "                        y2 = cy + h/2\n",
        "                        \n",
        "                        # Clamp coordinates to image bounds\n",
        "                        x1, x2 = max(0, x1), min(256, x2)\n",
        "                        y1, y2 = max(0, y1), min(256, y2)\n",
        "                        \n",
        "                        if x2 <= x1 or y2 <= y1:\n",
        "                            continue  # Skip invalid boxes\n",
        "                        \n",
        "                        boxes.append([x1, y1, x2, y2])\n",
        "                        scores.append(prob)\n",
        "                        labels.append(inverse_category_map[class_id])\n",
        "                \n",
        "                # Apply Non-Maximum Suppression\n",
        "                if len(boxes) > 0:\n",
        "                    boxes_tensor = torch.tensor(boxes)\n",
        "                    scores_tensor = torch.tensor(scores)\n",
        "                    \n",
        "                    # Use NMS to filter boxes\n",
        "                    keep_idx = ops.nms(boxes_tensor, scores_tensor, iou_threshold=0.5)\n",
        "                    \n",
        "                    # Keep only surviving predictions\n",
        "                    final_boxes = boxes_tensor[keep_idx].tolist()\n",
        "                    final_scores = scores_tensor[keep_idx].tolist()\n",
        "                    final_labels = [labels[i] for i in keep_idx]\n",
        "                    \n",
        "                    # Format into prediction list\n",
        "                    img_preds = [\n",
        "                        {'label': lbl, 'box': box, 'confidence': conf}\n",
        "                        for box, lbl, conf in zip(final_boxes, final_labels, final_scores)\n",
        "                    ]\n",
        "                \n",
        "                # Print textual predictions\n",
        "                print(f\"\\nImage {b+1} Predictions:\")\n",
        "                if len(img_preds) == 0:\n",
        "                    print(\"No objects detected\")\n",
        "                else:\n",
        "                    for idx, pred in enumerate(img_preds, 1):\n",
        "                        print(f\"Object {idx}: {pred['label']}\")\n",
        "                        print(f\"  Box: {[round(coord, 1) for coord in pred['box']]}\")\n",
        "                        print(f\"  Confidence: {pred['confidence']:.2f}\")\n",
        "\n",
        "                # Convert image tensor to PIL for plotting\n",
        "                img = to_pil(images[b].cpu())\n",
        "\n",
        "                # Plotting\n",
        "                fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "                ax.imshow(img)\n",
        "\n",
        "                # Plot ground truth boxes (scaled to 256x256)\n",
        "                gt_boxes = targets[b]['boxes']\n",
        "                gt_labels = targets[b]['labels']\n",
        "                for box, label in zip(gt_boxes, gt_labels):\n",
        "                    x1, y1, x2, y2 = box\n",
        "                    # Handle scaling if necessary\n",
        "                    if x1 > 256 or x2 > 256 or y1 > 256 or y2 > 256:\n",
        "                        original_width, original_height = targets[b]['original_size']\n",
        "                        x1 = x1 * 256 / original_width\n",
        "                        x2 = x2 * 256 / original_width\n",
        "                        y1 = y1 * 256 / original_height\n",
        "                        y2 = y2 * 256 / original_height\n",
        "                    rect = patches.Rectangle(\n",
        "                        (x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='g', facecolor='none'\n",
        "                    )\n",
        "                    ax.add_patch(rect)\n",
        "                    ax.text(x1, y1 - 5, label, color='green', fontsize=10, \n",
        "                            bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "                # Plot predicted boxes\n",
        "                for pred in img_preds:\n",
        "                    x1, y1, x2, y2 = pred['box']\n",
        "                    rect = patches.Rectangle(\n",
        "                        (x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='r', facecolor='none'\n",
        "                    )\n",
        "                    ax.add_patch(rect)\n",
        "                    ax.text(\n",
        "                        x1, y1 - 5, f\"{pred['label']} ({pred['confidence']:.2f})\",\n",
        "                        color='red', fontsize=10, bbox=dict(facecolor='white', alpha=0.5)\n",
        "                    )\n",
        "\n",
        "                ax.set_title(f\"Image {len(predictions) + 1}: Ground Truth (Green) vs Predicted (Red)\")\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "\n",
        "                predictions.append({\n",
        "                    'image_idx': b,\n",
        "                    'predictions': img_preds,\n",
        "                    'ground_truth': targets[b]\n",
        "                })\n",
        "\n",
        "            break  # Process only one batch for visualization\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d99b691-ac6d-4e96-98f7-80e2d6a0c275",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running inference and visualization on test set...\n"
          ]
        },
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run inference and visualize\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning inference and visualization on test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m perform_inference_and_visualize(\u001b[43mmodel\u001b[49m, test_dataloader, confidence_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, num_images_to_show\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTotal images processed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# Run inference and visualize\n",
        "print(\"Running inference and visualization on test set...\")\n",
        "results = perform_inference_and_visualize(model, test_dataloader, confidence_threshold=0.5, num_images_to_show=3)\n",
        "\n",
        "print(f\"\\nTotal images processed: {len(results)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc94c9bd-6ac7-4dc2-b3d8-886277e3eaa9",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
